{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ddi3-frREsTK",
        "outputId": "ee2723dc-d3c9-4985-ab37-7dce36d9309b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        }
      ],
      "source": [
        "%pylab inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCsbC4TBEsTM"
      },
      "outputs": [],
      "source": [
        "# import the libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn import neighbors\n",
        "from sklearn import tree\n",
        "from sklearn.ensemble import RandomForestClassifier as RF\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_curve, auc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ha_mEHYEsTM"
      },
      "source": [
        "***Clean the data***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "-4xBP_esEsTN",
        "outputId": "cebb6f8b-45ed-4994-a7d8-1845a117ba9f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-2b96b137e424>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# read the data and normilize it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'main_50k_pixels.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin-1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\";\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'main_50k_pixels.csv'"
          ]
        }
      ],
      "source": [
        "# read the data and normilize it\n",
        "df=pd.read_csv('main_50k_pixels.csv',encoding='latin-1', delimiter=\";\", low_memory=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21B4Zna1EsTO"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYhGde99EsTO"
      },
      "outputs": [],
      "source": [
        "del df['luc']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkPhK915EsTO"
      },
      "outputs": [],
      "source": [
        "df_norm=df.convert_objects(convert_numeric=True).apply(lambda x: (x-x.min())/np.ptp(x)).dropna(how='any', axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fn4WXv5cEsTP"
      },
      "outputs": [],
      "source": [
        "df_norm.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-_zJsqtEsTP"
      },
      "source": [
        "###df.columns.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pMszGPdEsTP"
      },
      "outputs": [],
      "source": [
        "# create 3 subsets for different years\n",
        "df1=df_norm[df_norm.columns.drop(list(df_norm.filter(regex='id2|Unnamed: 0|pobtot1973|vivocupada1973|2000|dfedge.86|dfedge.97|for.86|for.97|pro.aft80per|popgrowth')))]\n",
        "df2=df_norm[df_norm.columns.drop(list(df_norm.filter(regex='id2|Unnamed: 0|pobtot1973|vivocupada1973|2000|dfedge.60|dfedge.97|for.60|for.97|pro.aft80per|popgrowth')))]\n",
        "df3=df_norm[df_norm.columns.drop(list(df_norm.filter(regex='id2|Unnamed: 0|poptot2000|pobtot1973|empty.2000|pop.2000|73|1960|dfedge.60|dfedge.86|for.60|for.86|pro.bef80per|pro.before2000per|popgrowth')))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "45u7-ZUIEsTP"
      },
      "outputs": [],
      "source": [
        "print(df1.shape)\n",
        "print(df2.shape)\n",
        "print(df3.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCl8G-FqEsTQ"
      },
      "outputs": [],
      "source": [
        "F1 = df1['for.60']\n",
        "F2 = df2['for.86']\n",
        "F3 = df3['for.97']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEwPjgxVEsTQ"
      },
      "outputs": [],
      "source": [
        "(df1['for.60'] != 0).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HfxdqH9EsTQ"
      },
      "outputs": [],
      "source": [
        "(df1['for.60'] != 1).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kp8W7ACsEsTQ"
      },
      "outputs": [],
      "source": [
        "(df2['for.86'] != 0).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Aei0lgAEsTQ"
      },
      "outputs": [],
      "source": [
        "(df2['for.86'] != 1).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bt880hs1EsTR"
      },
      "outputs": [],
      "source": [
        "(df3['for.97'] != 1).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z01zFovuEsTR"
      },
      "outputs": [],
      "source": [
        "(df3['for.97'] != 0).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6HklYA8EsTR"
      },
      "outputs": [],
      "source": [
        "# Rearrange the data to have predictor as a first column\n",
        "df1.drop(labels=['for.60'], axis=1,inplace = True)\n",
        "df1.insert(0, 'for.60', F1)\n",
        "df2.drop(labels=['for.86'], axis=1,inplace = True)\n",
        "df2.insert(0, 'for.86', F2)\n",
        "df3.drop(labels=['for.97'], axis=1,inplace = True)\n",
        "df3.insert(0, 'for.97', F3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7RItnmQEsTR"
      },
      "outputs": [],
      "source": [
        "print(df1.shape)\n",
        "print(df2.shape)\n",
        "print(df3.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jR9p8ypEsTR"
      },
      "outputs": [],
      "source": [
        "df.head(15)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ibt2RjrKEsTS"
      },
      "source": [
        "***Classification***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MXVmOS8EsTS"
      },
      "outputs": [],
      "source": [
        "# Choose the classification methods\n",
        "\n",
        "X1 = df1.values\n",
        "X2 = df2.values\n",
        "X3 = df3.values\n",
        "\n",
        "gnb = GaussianNB()\n",
        "qda = QDA()\n",
        "lda = LDA()\n",
        "knn = neighbors.KNeighborsClassifier(n_neighbors = 3)\n",
        "dt = tree.DecisionTreeClassifier(criterion= 'gini', max_depth= None)\n",
        "rf = RF(criterion= 'gini')\n",
        "\n",
        "names = ['GaussianNB', 'QDA', 'LDA', 'kNN','Decision Tree','Random Forest']\n",
        "classifiers = [gnb, qda, lda, knn, dt, rf]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDoODaEREsTS"
      },
      "outputs": [],
      "source": [
        "print(X1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSymjr9pEsTS"
      },
      "outputs": [],
      "source": [
        "# Classification within the same year\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(X1[:,1:54], X1[:,0], test_size=0.33,random_state=1)\n",
        "\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2[:,1:54], X2[:,0], test_size=0.33,random_state=1)\n",
        "X_train3, X_test3, y_train3, y_test3 = train_test_split(X3[:,1:54], X3[:,0], test_size=0.33,random_state=1)\n",
        "\n",
        "for name, classifier in zip(names, classifiers):\n",
        "    classifier.fit(X_train1, y_train1)\n",
        "    pred1 = classifier.predict(X_test1)\n",
        "    print('{} -- Number of mislabeled points out of total {} points: {}'.format(name, len(y_test1), (pred1 != y_test1).sum()))\n",
        "print()\n",
        "for name, classifier in zip(names, classifiers):\n",
        "    classifier.fit(X_train2, y_train2)\n",
        "    pred2 = classifier.predict(X_test2)\n",
        "    print('{} -- Number of mislabeled points out of total {} points: {}'.format(name, len(y_test2), (pred2 != y_test2).sum()))\n",
        "print()\n",
        "for name, classifier in zip(names, classifiers):\n",
        "    classifier.fit(X_train3, y_train3)\n",
        "    pred3 = classifier.predict(X_test3)\n",
        "    print('{} -- Number of mislabeled points out of total {} points: {}'.format(name, len(y_test3), (pred3 != y_test3).sum()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJLpigN-EsTS"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix and prediction rate\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix (y_test1,pred1))\n",
        "print ('Predicted\\nError Rate = {:0.2f}%'.format((pred1 != y_test1).sum()*100/len(y_test1)))\n",
        "print(confusion_matrix (y_test2,pred2))\n",
        "print ('Predicted\\nError Rate = {:0.2f}%'.format((pred2 != y_test2).sum()*100/len(y_test2)))\n",
        "print(confusion_matrix (y_test3,pred3))\n",
        "print ('Predicted\\nError Rate = {:0.2f}%'.format((pred3 != y_test3).sum()*100/len(y_test3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TI7m1_H_EsTT"
      },
      "source": [
        "***Prediction***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtpJQBPZEsTT"
      },
      "source": [
        "*Prediction of 1980 using 1960*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbqX4yJZEsTT"
      },
      "outputs": [],
      "source": [
        "# Prediction of 1980 using 1960\n",
        "X_train, X_test, y_train, y_test = train_test_split(X1[:,1:54], X2[:,0], test_size=0.33,random_state=1)\n",
        "\n",
        "\n",
        "\n",
        "for name, classifier in zip(names, classifiers):\n",
        "    classifier.fit(X_train, y_train)\n",
        "    pred = classifier.predict(X_test)\n",
        "    print('{} -- Number of mislabeled points out of total {} points: {}'.format(name, len(y_test), (pred != y_test).sum()))\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvZ3JNNpEsTT"
      },
      "outputs": [],
      "source": [
        "print ('Predicted\\nError Rate = {:0.2f}%'.format((pred != y_test).sum()*100/len(y_test)))\n",
        "print(confusion_matrix (y_test,pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzVaA-1HEsTT"
      },
      "outputs": [],
      "source": [
        "# Decision tree best predictors\n",
        "feat_importances1 = pd.Series(dt.feature_importances_, index=df1.columns[1:54])\n",
        "feat_importances1.nlargest(20).plot(kind='barh')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lB8YMmNrEsTT"
      },
      "outputs": [],
      "source": [
        "# Random forest best predictors\n",
        "feat_importances = pd.Series(rf.feature_importances_, index=df1.columns[1:54])\n",
        "feat_importances.nlargest(20).plot(kind='barh')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVMLd-GwEsTT"
      },
      "outputs": [],
      "source": [
        "# Random forest best predictors\n",
        "feat_importances = pd.Series(rf.feature_importances_, index=df1.columns[1:54])\n",
        "feat_importances.nlargest(7).plot(kind='barh')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "2n-YOJldEsTU"
      },
      "outputs": [],
      "source": [
        "# Best 7 predictors of RF\n",
        "feat_importances.nlargest(7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQIAhD6qEsTU"
      },
      "source": [
        "***Prediction of 1980 with the best 7 predictors ***\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCztF9fAEsTU"
      },
      "outputs": [],
      "source": [
        "# New data set that only contains best predictive variables from 1960\n",
        "new_df1 = df1[['drd.00','drd','dfedge.60','for60per','dmcity','seg.area','slope']].copy()\n",
        "new_df1[:4]\n",
        "new_df1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFY1HwIoEsTU"
      },
      "outputs": [],
      "source": [
        "X11 = new_df1.values\n",
        "# Prediction of 1980 using 1960\n",
        "XX_train, XX_test, yy_train, yy_test = train_test_split(X11[:,1:9], X2[:,0], test_size=0.33,random_state=1)\n",
        "\n",
        "\n",
        "\n",
        "for name, classifier in zip(names, classifiers):\n",
        "    classifier.fit(XX_train, yy_train)\n",
        "    pred4 = classifier.predict(XX_test)\n",
        "    print('{} -- Number of mislabeled points out of total {} points: {}'.format(name, len(yy_test), (pred4 != yy_test).sum()))\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kRohaG6EsTU"
      },
      "outputs": [],
      "source": [
        "print ('Predicted\\nError Rate = {:0.2f}%'.format((pred4 != yy_test).sum()*100/len(yy_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqF7fAieEsTU"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "\n",
        "confusion_matrix (yy_test,pred4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnRIFYLnEsTU"
      },
      "source": [
        "*Prediction of 2000 using the previous model*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLEKvlxBEsTU"
      },
      "outputs": [],
      "source": [
        "# Prediction of 2000 using 1980\n",
        "X60_train = X1[:,1:54]\n",
        "Y80_train = X2[:,0]\n",
        "X80_test = X2[:,1:54]\n",
        "Y2000_test = X3[:,0]\n",
        "\n",
        "for name, classifier in zip(names, classifiers):\n",
        "    classifier.fit(X60_train, Y80_train)\n",
        "    pred2000 = classifier.predict(X80_test)\n",
        "    print('{} -- Number of mislabeled points out of total {} points: {}'.format(name, len(Y2000_test), (pred2000 != Y2000_test).sum()))\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOcLGwKpEsTV"
      },
      "outputs": [],
      "source": [
        "confusion_matrix (Y2000_test,pred2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qt-Ov65-EsTV"
      },
      "outputs": [],
      "source": [
        "print ('Predicted\\nError Rate = {:0.2f}%'.format((pred2000 != Y2000_test).sum()*100/len(Y2000_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsVJi6yFEsTV"
      },
      "source": [
        "*ROC Curve*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVb3A1axEsTV"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.seed(123)\n",
        "random.shuffle(X1)\n",
        "random.shuffle(X11)\n",
        "\n",
        "XX1 = X1[:int(len(X1.T))]\n",
        "XX2 = X11[:int(len(X11.T))]\n",
        "print(XX1.shape)\n",
        "print(XX2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OirysuaJEsTV"
      },
      "outputs": [],
      "source": [
        "#plot orig and new covariance matrices(estimate w/o norm)\n",
        "figure(figsize=(10,5))\n",
        "subplot(121);imshow(XX1.dot(XX1.T),interpolation = 'none',cmap = 'viridis');\n",
        "plt.title('Covariance Matrix for 1960')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kb3F2cAdEsTV"
      },
      "outputs": [],
      "source": [
        "figure(figsize=(10,5))\n",
        "subplot(122);imshow(XX2.dot(XX2.T),interpolation = 'none',cmap = 'viridis');\n",
        "plt.title('Covariance Matrix for best 7 Predictors')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJI7ydJiEsTW"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "fpr, tpr, thresholds = roc_curve(pred2000, Y2000_test)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure()\n",
        "lw = 2\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=1, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic for 2000 forestation prediction')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}